2 fully connected layers: 1000 -> 512, 512 -> 128
Training parameters: lr: 0.008223451079092928, betas: (0.8815565440583507, 0.9798388065714719), batch size: 4 for 0.6

